# ğŸ§  Week 4 â€“ Session 3: Red Teaming Methodologies & MAS Penetration Testing

## ğŸ¯ Objective
Simulate a **penetration test** on a multi-agent system (MAS) by **injecting altered (tampered) policies** and observing how the network behaves under stress.  
Youâ€™ll perform:
1. **Baseline run** â€“ cooperative agents performing normally.  
2. **Adversarial run** â€“ one agent replaced with a tampered policy.  
3. **Stress test** â€“ repeat experiments with different seeds and analyse robustness.  

This lab demonstrates **model-supply-chain attacks**, **adversarial reinforcement learning**, and how **distributed learning breaks down** under coordinated perturbations.

---

## ğŸ§© Environment Setup

### ğŸ”§ Prerequisites
- Python 3.10+  
- VS Code or any IDE  
- No GPU required  
- Linux/macOS/Windows (Docker optional)  

### ğŸª„ Installation
```bash
git clone <your-repo-url>
cd session3_redteam_lab
python3 -m venv .venv
# Linux/macOS
. .venv/bin/activate
# Windows (PowerShell)
.venv\Scripts\Activate
pip install -r requirements.txt

ğŸ§± Requirements

requirements.txt

pettingzoo>=1.24
gymnasium>=0.29
numpy>=1.24
torch>=2.0.0
networkx>=3.2
pandas>=2.0
matplotlib>=3.5
tqdm>=4.65


â¸»

ğŸ§  Concept Overview
	â€¢	Baseline policies â†’ clean cooperative models.
	â€¢	Adversarial policy â†’ same architecture but weight tampering simulates registry compromise.
	â€¢	Injection â†’ replace one agentâ€™s model during runtime.
	â€¢	Stress test â†’ run multiple episodes to measure systemic degradation.

Metrics include:
	â€¢	Mean team reward
	â€¢	Reward variance
	â€¢	Task completion rate
	â€¢	Recovery time

â¸»

ğŸ“‚ Repository Layout

session3_redteam_lab/
â”œâ”€â”€ baseline_policy.py        # creates baseline policies
â”œâ”€â”€ perturb_policy.py         # generates adversarial policy
â”œâ”€â”€ run_injection.py          # runs MAS with injected policy
â”œâ”€â”€ stress_test.py            # ensemble tests and metrics
â”œâ”€â”€ analysis.ipynb            # notebook for plotting results
â”œâ”€â”€ utils.py                  # helper utilities
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


â¸»

ğŸš€ Step-by-Step Guide

1ï¸âƒ£ Create Baseline Policies

Generate simple cooperative policies and store them under /policies.

python baseline_policy.py

These act as â€œknown goodâ€ models.

â¸»

2ï¸âƒ£ Create an Adversarial Policy

Perturb one agentâ€™s weights to simulate a tampered model in your supply chain.

python perturb_policy.py policies/baseline_agent_2.pt policies/adversarial_agent_2.pt --noise 0.25

This adds Gaussian noise (0.25Ïƒ) to the policyâ€™s parameters.

â¸»

3ï¸âƒ£ Run an Injected Episode

Run the environment where Agent 2 is replaced with the adversarial model.

python run_injection.py --adversary-agent 2 --adversary-policy policies/adversarial_agent_2.pt --seed 0 --out episode_injected.csv

Outputs:
	â€¢	episode_injected.csv â†’ step-level rewards
	â€¢	Logs showing average reward fluctuations

â¸»

4ï¸âƒ£ Run Ensemble Stress Test

Execute 30 episodes (baseline vs adversarial) and collect robustness metrics.

python stress_test.py --episodes 30 --adversary-agent 2 --adversary-policy policies/adversarial_agent_2.pt --seed 0 --out stress_results.csv

Outputs:
	â€¢	stress_results.csv â†’ aggregated metrics for baseline vs injected runs
	â€¢	Console logs showing per-episode summaries

â¸»

5ï¸âƒ£ Analyse Results

Open analysis.ipynb in Jupyter or VS Code.

Example code:

import pandas as pd, matplotlib.pyplot as plt
df = pd.read_csv("stress_results.csv")
df.boxplot(column="mean_avg_reward", by="condition", grid=False)
plt.title("MAS Robustness under Adversarial Injection")
plt.suptitle("")
plt.ylabel("Mean Average Reward")
plt.show()

Interpretation:
	â€¢	Box height â†’ variability across episodes
	â€¢	Drop in average reward â†’ loss of coordination
	â€¢	Wider spread â†’ instability introduced by adversary

â¸»

ğŸ§ª Mitigation Experiments

Try these defences and rerun the stress test:

Mitigation	Implementation	Expected Effect
Reward Clipping	Limit reward to [-1, 1] in run_injection.py.	Reduces reward-exploitation impact.
Majority Consensus	Drop messages when <50% neighbours agree (simulated).	Stops single-agent belief poisoning.
Message Verification	Add signed=True flag to messages; ignore unsigned.	Prevents agent impersonation.

Run again:

python stress_test.py --episodes 30 --adversary-agent 2 --adversary-policy policies/adversarial_agent_2.pt

Compare new results with stress_results.csv baseline.

â¸»

ğŸ“Š Expected Output
	â€¢	Baseline condition: stable mean rewards (flat distribution).
	â€¢	Injected condition: reward drop of 25â€“50%, increased variance.
	â€¢	Mitigated condition: partial recovery, reduced variance.

Example summary (typical CPU run):

Condition	Mean Reward	Std Dev	Relative Loss
Baseline	0.72	0.08	â€”
Injected	0.41	0.23	43% â†“
Mitigated	0.63	0.12	12% â†“


â¸»

ğŸ§© Ethical and Safety Considerations
	â€¢	Run only in isolated environments (no live systems).
	â€¢	Use for educational red-team simulations, not production testing.
	â€¢	Always disclose findings under responsible-AI guidelines.
	â€¢	Delete adversarial files (adversarial_agent_*.pt) after completion.

â¸»

ğŸ“˜ Deliverables for Participants
	1.	stress_results.csv â€” your quantitative comparison file.
	2.	Plots of baseline vs injected mean rewards.
	3.	Short (1â€“2 paragraph) report answering:
	â€¢	How severe was the degradation?
	â€¢	Which mitigation helped most?
	â€¢	How does this map to real-world MAS architectures (e.g., drones, trading agents, chat-based coordinators)?

â¸»

ğŸ’¡ Optional Extension
	â€¢	Replace linear policies with 2-layer MLPs.
	â€¢	Implement digital-signature verification before loading any policy.
	â€¢	Integrate with a mock SLSA (Supply-chain Levels for Software Artifacts) attestation gate to reject unsigned models.

â¸»

ğŸ“ Reference

This lab corresponds to Session 3, Week 4 of the Agentic AI Security Bootcamp.
Slides: /mnt/data/27OCT2025_Agentic AI Security Bootcamp.pptx
Frameworks: MITRE ATLASï¿¼ Â· STRIDEï¿¼
Environment: PettingZooï¿¼ Â· Gymnasiumï¿¼

â¸»

âœ… Outcome

By completing this lab, you will:
	â€¢	Understand how adversarial policies disrupt cooperation.
	â€¢	Measure MAS resilience quantitatively.
	â€¢	Apply red-teaming workflows to AI infrastructures.
	â€¢	Propose effective policy-level mitigations and governance strategies.

â¸»



